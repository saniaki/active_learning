{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy.stats import norm\n",
    "from scipy import linalg\n",
    "from scipy.stats import halfnorm\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import clear_output\n",
    "import timeit\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set figure defaults for IPython notebook \n",
    "#matplotlib.rcParams.update({'font.size': 18, 'lines.linewidth':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.FETemp import FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(start_t,end_t,step_t,t_out,X_true,obs,Sample,pred):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(list(np.arange(start_t,end_t+step_t,step_t)),X_true, color='red', linewidth = 2, label = 'True X')\n",
    "    plt.scatter(list(np.arange(start_t,end_t+step_t,step_t)),obs, color='blue', label = 'Observations')\n",
    "    plt.plot(list(np.arange(start_t,end_t+step_t,step_t)),pred[1:], color='green', linewidth = 2, label = 'Predictions')\n",
    "    print('Average error between true X and observations is:', round(np.sum(abs(obs-X_true))/np.sum(abs(X_true))*100,2))\n",
    "    print('Average error between true X and predicted values is:', round(np.sum(abs(pred[1:]-X_true))/np.sum(abs(X_true))*100,2))\n",
    "    plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    histogram = plt.hist(Sample[:,int((t_out-start_t)/step_t+1)], bins=int(N/100), label = \"Distribution at time {}\".format(t_out))\n",
    "    x_true_t = X_true[int((t_out-start_t)/step_t+1)] # true value at time t_out\n",
    "    obs_t = obs[int((t_out-start_t)/step_t+1)] # true value at time t_out\n",
    "    pred_t = pred[int((t_out-start_t)/step_t+1)] # prediction value at time t_out\n",
    "    plt.plot([x_true_t,x_true_t],[0,100], color='red', linewidth = 3, label = \"True value at time {}\".format(t_out))\n",
    "    plt.plot([obs_t,obs_t],[0,100], color='black', linewidth = 3, label = \"Observation at time {}\".format(t_out))\n",
    "    plt.plot([pred_t,pred_t],[0,100], color='green', linewidth = 3, label = \"Distribution mean at time {}\".format(t_out))\n",
    "    plt.legend(bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T_true is an array with shape (number_node,number_of time steps) coming from FE solution for the entire time\n",
    "# sensor_loc is a list of sensor locations with size (number_sensors)\n",
    "\n",
    "def Temp_bootstrap(sensor_loc_typ,sensor_loc_list,obs_sigma,T_sigma,alpha_sigma,alpha_dot_sigma,\n",
    "                   muT_sigma,mualpha_sigma,mualpha_dot_sigma,\n",
    "                   N,t_start,t_end,delt,Length_c,Length_t,num_el_c,num_el_t,\n",
    "                   Coords_start,air_temp_type,T_start,T_hold,T_const,T_rate,th1,th2,T_air_sigma,\n",
    "                   material_dict,\n",
    "                   Analysis,cri,Element_type,heat_gen,T_true,alpha_true,alpha_dot_true):\n",
    "\n",
    "    Length = Length_c + Length_t\n",
    "    num_el = num_el_c + num_el_t\n",
    "    # material properties sampling\n",
    "    # sampling\n",
    "    k_c = np.random.normal(material_dict['k_c_mean'],material_dict['k_c_sigma'],N)\n",
    "    rho_c = np.random.normal(material_dict['rho_c_mean'],material_dict['rho_c_sigma'],N)\n",
    "    Cp_c = np.random.normal(material_dict['Cp_c_mean'],material_dict['Cp_c_sigma'],N)\n",
    "    rho_r = np.random.normal(material_dict['rho_r_mean'],material_dict['rho_r_sigma'],N)\n",
    "    H_r = np.random.normal(material_dict['H_r_mean'],material_dict['H_r_sigma'],N)\n",
    "    nu_r = np.random.normal(material_dict['nu_r_mean'],material_dict['nu_r_sigma'],N)\n",
    "    h_c = np.random.normal(material_dict['h_c_mean'],material_dict['h_c_sigma'],N)\n",
    "    \n",
    "    k_t = np.random.normal(material_dict['k_t_mean'],material_dict['k_t_sigma'],N)\n",
    "    rho_t = np.random.normal(material_dict['rho_t_mean'],material_dict['rho_t_sigma'],N)\n",
    "    Cp_t = np.random.normal(material_dict['Cp_t_mean'],material_dict['Cp_t_sigma'],N)\n",
    "    h_t = np.random.normal(material_dict['h_t_mean'],material_dict['h_t_sigma'],N)\n",
    "    \n",
    "    # particles in FE\n",
    "    A1 = np.random.normal(material_dict['A1_mean'],material_dict['A1_sigma'],N)\n",
    "    A2 = np.random.normal(material_dict['A2_mean'],material_dict['A2_sigma'],N)\n",
    "    A3 = np.random.normal(material_dict['A3_mean'],material_dict['A3_sigma'],N)\n",
    "    dE1 = np.random.normal(material_dict['dE1_mean'],material_dict['dE1_sigma'],N)\n",
    "    dE2 = np.random.normal(material_dict['dE2_mean'],material_dict['dE2_sigma'],N)\n",
    "    dE3 = np.random.normal(material_dict['dE3_mean'],material_dict['dE3_sigma'],N)\n",
    "    BB = np.random.normal(material_dict['BB_mean'],material_dict['BB_sigma'],N)\n",
    "    a_c =  k_c/(rho_c*Cp_c)\n",
    "    b_c =  rho_r*H_r*nu_r/(rho_c*Cp_c)\n",
    "    Ch_c = h_c/k_c*a_c \n",
    "    a_t =  k_t/(rho_t*Cp_t)\n",
    "    b_t =  np.zeros(N,)\n",
    "    Ch_t = h_t/k_t*a_t\n",
    "       \n",
    "    \n",
    "    n = int(int(t_end-t_start)/delt + 1) # number of states\n",
    "    \n",
    "    if sensor_loc_typ == \"node\":\n",
    "        sensor_loc_n = sensor_loc_list # a list, node numbers\n",
    "        sensor_loc = [(i-1) * (Length/num_el) for i in sensor_loc_n] \n",
    "    elif sensor_loc_typ == \"loc\":\n",
    "        sensor_loc = sensor_loc_list # a list, location of sensor (m)\n",
    "        sensor_loc_n = [int(round(x /  (Length/num_el))) + 1 for x in sensor_loc] # sensor location node number\n",
    "\n",
    "    # Generating fake observations from T_true\n",
    "    # observations is an array with shape (number_sensors,number_timestep)\n",
    "    observations = np.zeros((len(sensor_loc_n),n)) # n is the number of time steps\n",
    "    for sens in range(len(sensor_loc_n)): # observations if we put the sensor at i location\n",
    "        observations[sens,:]  = T_true[sensor_loc_n[sens]-1,:] + np.random.normal(0,obs_sigma,n) \n",
    "        \n",
    "    # initialization, t=0\n",
    "    T_0_allp = np.ones((1,N)) \n",
    "    for node in range(0,num_el+1):\n",
    "        muT_mean = T_true[node,0]\n",
    "        T_0 = np.random.normal(muT_mean,muT_sigma,N) # N samples from mu ~ Normal(mu_mean,mu_sigma)\n",
    "        T_0_allp = np.append(T_0_allp,T_0.reshape(1,N), axis=0)  \n",
    "    T_old_allp = T_0_allp[1:,:]\n",
    "    T_all_ave =np.mean(T_old_allp,axis=1).reshape(num_el+1,1) #np.zeros((num_el+1,1))\n",
    "    T_all_var = np.zeros((num_el+1,1))\n",
    "    T_all_var.fill(muT_sigma)\n",
    "    \n",
    "    alpha_0_allp = np.ones((1,N)) \n",
    "    alpha_dot_0_allp = np.ones((1,N)) \n",
    "    for el in range(0,num_el):\n",
    "        mualpha_mean = alpha_true[el,0]\n",
    "        alpha_0 = np.random.normal(mualpha_mean,mualpha_sigma,N) # N samples from mu ~ Normal(mu_mean,mu_sigma)\n",
    "        alpha_0_allp = np.append(alpha_0_allp,alpha_0.reshape(1,N), axis=0) \n",
    "        mualpha_dot_mean = alpha_dot_true[el,0]\n",
    "        #alpha_dot_0 = np.random.normal(mualpha_dot_mean,mualpha_dot_sigma,N) # N samples from mu ~ Normal(mu_mean,mu_sigma)\n",
    "        alpha_dot_0 = halfnorm.rvs(loc = mualpha_dot_mean, scale = mualpha_dot_sigma, size = N)\n",
    "        alpha_dot_0_allp = np.append(alpha_dot_0_allp,alpha_dot_0.reshape(1,N), axis=0) \n",
    "    alpha_old_allp = alpha_0_allp[1:,:]\n",
    "    alpha_all_ave =np.mean(alpha_old_allp,axis=1).reshape(num_el,1)  # np.zeros((num_el,1))\n",
    "    alpha_all_var =np.zeros((num_el,1))\n",
    "    alpha_all_var.fill(mualpha_sigma)\n",
    "    alpha_dot_old_allp = alpha_dot_0_allp[1:,:]\n",
    "    alpha_dot_all_ave = np.mean(alpha_dot_old_allp,axis=1).reshape(num_el,1) # np.zeros((num_el,1))\n",
    "    alpha_dot_all_var =np.zeros((num_el,1))\n",
    "    alpha_dot_all_var.fill(mualpha_dot_sigma)\n",
    "    \n",
    "    for t in np.arange(t_start,t_end,delt):\n",
    "        # Solve one step of FE for each particle to obtain new T_mean\n",
    "        T_mean_allp = np.zeros((num_el+1,1))\n",
    "        alpha_mean_allp = np.zeros((num_el,1))\n",
    "        alpha_dot_mean_allp = np.zeros((num_el,1))\n",
    "        for p in range(0,N):\n",
    "            T_mean, Coords, alpha_mean, alpha_dot_mean = FE(t,t+delt,delt,Length_c,Length_t,num_el_c,num_el_t,\n",
    "                                                            Coords_start,\n",
    "                                                            air_temp_type,T_start,T_hold,\n",
    "                                                            T_const,T_rate,th1,th2,T_air_sigma,\n",
    "                                                            a_c[p],b_c[p],Ch_c[p],a_t[p],b_t[p],Ch_t[p],\n",
    "                                                            BB[p],A1[p],A2[p],A3[p],dE1[p],dE2[p],dE3[p],\n",
    "                                                            Analysis,cri,\n",
    "                                                            Element_type,heat_gen,\n",
    "                                                            T_old_allp[:,p].reshape(num_el+1,1),\n",
    "                                                            alpha_old_allp[:,p].reshape(num_el,1),\n",
    "                                                            alpha_dot_old_allp[:,p].reshape(num_el,1))\n",
    "            \n",
    "            T_mean_allp = np.append(T_mean_allp,T_mean[:,1].reshape(num_el+1,1),axis=1)\n",
    "            alpha_mean_allp = np.append(alpha_mean_allp,alpha_mean[:,1].reshape(num_el,1),axis=1)\n",
    "            alpha_dot_mean_allp = np.append(alpha_dot_mean_allp,alpha_dot_mean[:,1].reshape(num_el,1),axis=1)\n",
    "            \n",
    "        T_mean_allp = T_mean_allp[:,1:]\n",
    "        alpha_mean_allp = alpha_mean_allp[:,1:]\n",
    "        alpha_dot_mean_allp = alpha_dot_mean_allp[:,1:]\n",
    "\n",
    "        # Sampling the new particles for each node/element\n",
    "        T_new_allp = np.zeros((1,N))\n",
    "        for node in range(0,num_el+1):\n",
    "            T_new_node = np.random.normal(T_mean_allp[node,:],T_sigma,N)\n",
    "            T_new_allp = np.append(T_new_allp,T_new_node.reshape(1,N), axis=0)\n",
    "        alpha_new_allp = np.zeros((1,N))\n",
    "        alpha_dot_new_allp = np.zeros((1,N))\n",
    "        for el in range(0,num_el):\n",
    "            alpha_new_el =  np.random.normal(alpha_mean_allp[el,:],alpha_sigma,N) # alpha_mean_allp[el,:]\n",
    "            alpha_new_allp = np.append(alpha_new_allp,alpha_new_el.reshape(1,N), axis=0)\n",
    "            #alpha_dot_new_el = np.random.halfnormal(alpha_dot_mean_allp[el,:],alpha_dot_sigma,N) # alpha_dot_mean_allp[el,:] \n",
    "            alpha_dot_new_el = halfnorm.rvs(loc = alpha_dot_mean_allp[el,:], scale = alpha_dot_sigma, size = N)\n",
    "            alpha_dot_new_allp = np.append(alpha_dot_new_allp,alpha_dot_new_el.reshape(1,N), axis=0)\n",
    "             \n",
    "        \n",
    "        # weight calculations\n",
    "        Weight_allp = np.zeros((1,N))\n",
    "        for sens in range(len(sensor_loc_n)): # len(sensor_loc_n) = number of srensors\n",
    "            tn = int((t-t_start)/delt) # time step number\n",
    "            weight = sp.stats.norm.pdf(observations[sens,tn], T_new_allp[sensor_loc_n[sens],:], obs_sigma) # sp.stats.norm.pdf(observation[node], T_new_allp[node,:], obs_sigma) \n",
    "            Weight = weight / sum(weight) # normalizing the weights\n",
    "            Weight_allp = np.append(Weight_allp,Weight.reshape(1,N), axis=0)\n",
    "        Weight_allp = Weight_allp[1:,:]\n",
    "\n",
    "        # Resampling\n",
    "        s = 0\n",
    "        for i in range(len(sensor_loc_n)):\n",
    "            T_new_allp[sensor_loc_n[i],:] = np.random.choice(T_new_allp[sensor_loc_n[i],:], N, p=Weight_allp[s,:])\n",
    "            s +=1\n",
    "\n",
    "        # updating results\n",
    "        T_old_allp = T_new_allp[1:,:]\n",
    "        T_old_ave = np.mean(T_old_allp,axis=1)\n",
    "        T_old_var = np.var(T_old_allp, axis=1) \n",
    "        T_all_ave = np.append(T_all_ave,T_old_ave.reshape(num_el+1,1), axis=1)\n",
    "        T_all_var = np.append(T_all_var,T_old_var.reshape(num_el+1,1), axis=1)\n",
    "        \n",
    "        alpha_old_allp = alpha_new_allp[1:,:]\n",
    "        alpha_old_ave = np.mean(alpha_old_allp,axis=1)\n",
    "        alpha_old_var = np.var(alpha_old_allp, axis=1) \n",
    "        alpha_all_ave = np.append(alpha_all_ave,alpha_old_ave.reshape(num_el,1), axis=1)\n",
    "        alpha_all_var = np.append(alpha_all_var,alpha_old_var.reshape(num_el,1), axis=1)\n",
    "        \n",
    "        alpha_dot_old_allp = alpha_dot_new_allp[1:,:]\n",
    "        alpha_dot_old_ave = np.mean(alpha_dot_old_allp,axis=1)\n",
    "        alpha_dot_old_var = np.var(alpha_dot_old_allp, axis=1) \n",
    "        alpha_dot_all_ave = np.append(alpha_dot_all_ave,alpha_dot_old_ave.reshape(num_el,1), axis=1)\n",
    "        alpha_dot_all_var = np.append(alpha_dot_all_var,alpha_dot_old_var.reshape(num_el,1), axis=1)\n",
    "        \n",
    "        if int((t-t_start)/delt)%5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print (\"progress is : {}%\".format(round((t-t_start)/(t_end-t_start)*100,1)))\n",
    "        \n",
    "    #T_all_ave = T_all_ave[:,1:]\n",
    "    #T_all_var = T_all_var[:,1:]\n",
    "    \n",
    "    #alpha_all_ave = alpha_all_ave[:,1:]\n",
    "    #alpha_all_var = alpha_all_var[:,1:]\n",
    "    \n",
    "    #alpha_dot_all_ave = alpha_dot_all_ave[:,1:]\n",
    "    #alpha_dot_all_var = alpha_dot_all_var[:,1:]\n",
    "\n",
    "    return T_all_ave, T_all_var, Coords, alpha_all_ave, alpha_all_var, alpha_dot_all_ave, alpha_dot_all_var,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "159px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
